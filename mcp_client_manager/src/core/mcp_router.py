# llm_integration.py
import json
import os
from typing import Dict, Any, List
from openai import OpenAI


class LLMProtocolRouter:
    """LLMåè®®è·¯ç”±å™¨ - é€šè¿‡LLMå†³ç­–è°ƒç”¨å“ªä¸ªMCPåè®®å·¥å…·"""

    def __init__(self, mcp_client, api_key: str = None):
        self.mcp_client = mcp_client
        self.llm_client = OpenAI(api_key=api_key)
        self.model = "gpt-3.5-turbo"

    async def process_user_request(self, user_input: str) -> str:
        """å¤„ç†ç”¨æˆ·è¯·æ±‚ - é€šè¿‡LLMå†³ç­–ä½¿ç”¨å“ªä¸ªMCPå·¥å…·"""
        if not self.llm_client:
            return await self._fallback_process(user_input)

        try:
            # è·å–æ‰€æœ‰é€šè¿‡MCPåè®®æ³¨å†Œçš„å·¥å…·
            tools = self.mcp_client.get_all_tool_schemas()

            if not tools:
                return "é”™è¯¯: æ²¡æœ‰å¯ç”¨çš„MCPå·¥å…·"

            print(tools)
            # ä½¿ç”¨LLMè¿›è¡Œå·¥å…·è°ƒç”¨å†³ç­–
            response = self.llm_client.chat.completions.create(
                model=self.model,
                messages=[{
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨å„ç§å·¥å…·ã€‚è¯·åˆ†æç”¨æˆ·è¯·æ±‚å¹¶é€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚"
                }, {
                    "role": "user",
                    "content": user_input
                }],
                tools=tools,
                tool_choice="auto"
            )

            message = response.choices[0].message

            if message.tool_calls:
                results = []
                for tool_call in message.tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)

                    # é€šè¿‡MCPåè®®æ‰§è¡Œè°ƒç”¨
                    result = await self.mcp_client.call_tool(function_name, function_args)
                    results.append({
                        "tool": function_name,
                        "result": result
                    })

                return self._format_results(results)
            else:
                return message.content or "LLMå†³å®šç›´æ¥å›åº”"

        except Exception as e:
            print(f"LLMå†³ç­–å¤±è´¥: {e}")
            return await self._fallback_process(user_input)

    async def _fallback_process(self, user_input: str) -> str:
        """å›é€€å¤„ç† - åŸºäºå·¥å…·æè¿°çš„ç®€å•åŒ¹é…"""
        tools = self.mcp_client.get_all_tool_schemas()
        user_input_lower = user_input.lower()

        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        for tool in tools:
            function_info = tool["function"]
            tool_name = function_info["name"]
            description = function_info["description"].lower()

        return self._generate_help_response(tools)

    def _format_results(self, results: List[Dict]) -> str:
        """æ ¼å¼åŒ–å¤šä¸ªå·¥å…·è°ƒç”¨ç»“æœ"""
        formatted = []
        for result in results:
            formatted.append(f"ğŸ”§ {result['tool']}:\n{result['result']}")
        return "\n\n".join(formatted)

    def _generate_help_response(self, tools: List[Dict]) -> str:
        """ç”Ÿæˆå¸®åŠ©å“åº”"""
        help_lines = ["ğŸ¤– å¯ç”¨çš„MCPå·¥å…·:"]

        for tool in tools:
            function_info = tool["function"]
            help_lines.append(f"â€¢ {function_info['name']}: {function_info['description']}")

            # æ˜¾ç¤ºå‚æ•°ä¿¡æ¯
            params = function_info.get("parameters", {}).get("properties", {})
            for param_name, param_info in params.items():
                param_desc = param_info.get("description", "")
                help_lines.append(f"  â””â”€ {param_name}: {param_desc}")

        return "\n".join(help_lines)